{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "import re\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel, LdaModel\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path,file_name):\n",
    "    \"\"\"\n",
    "    Input  : path and file_name\n",
    "    Purpose: loading text file\n",
    "    Output : list of paragraphs/documents and\n",
    "             title(initial 100 words considred as title of document)\n",
    "    \"\"\"\n",
    "    documents_list = []\n",
    "    titles=[]\n",
    "    with open( os.path.join(path, file_name) ,\"r\", encoding=\"utf8\", errors=\"ignore\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            text = line.strip()\n",
    "            documents_list.append(text)\n",
    "    print(\"Total Number of Documents:\",len(documents_list))\n",
    "    titles.append( text[0:min(len(text),100)] )\n",
    "    return documents_list,titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(doc_set):\n",
    "    \"\"\"\n",
    "    Input  : docuemnt list\n",
    "    Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n",
    "    Output : preprocessed text\n",
    "    \"\"\"\n",
    "    # initialize regex tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # create English stop words list\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "    # loop through document list\n",
    "    for i in doc_set:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "        # stem tokens\n",
    "#         stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "        stemmed_tokens = [lemmatizer.lemmatize(i) for i in stopped_tokens]\n",
    "        # add tokens to list\n",
    "        texts.append(stemmed_tokens)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(doc_clean):\n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\n",
    "    Output : term dictionary and Document Term Matrix\n",
    "    \"\"\"\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    # generate LDA model\n",
    "    return dictionary,doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
    "    \"\"\"\n",
    "    Input  : clean document, number of topics and number of words associated with each topic\n",
    "    Purpose: create LSA model using gensim\n",
    "    Output : return LSA model\n",
    "    \"\"\"\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    # generate LSA model\n",
    "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
    "    return lsamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gensim_lda_model(doc_clean,number_of_topics,words):\n",
    "    \"\"\"\n",
    "    Input  : clean document, number of topics and number of words associated with each topic\n",
    "    Purpose: create LSA model using gensim\n",
    "    Output : return LSA model\n",
    "    \"\"\"\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    # generate LDA model\n",
    "    ldamodel = LdaModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "    print(ldamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
    "    return ldamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Input   : dictionary : Gensim dictionary\n",
    "              corpus : Gensim corpus\n",
    "              texts : List of input texts\n",
    "              stop : Max num of topics\n",
    "    purpose : Compute c_v coherence for various number of topics\n",
    "    Output  : model_list : List of LSA topic models\n",
    "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, stop, step):\n",
    "        # generate LSA model\n",
    "        model = LsiModel(doc_term_matrix, num_topics=num_topics, id2word = dictionary)  # train model\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(doc_clean,start, stop, step):\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary, doc_term_matrix,doc_clean,\n",
    "                                                            stop, start, step)\n",
    "    # Show graph\n",
    "    x = range(start, stop, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# document_list,titles=load_data(\"\",r\"C:\\Users\\Sourav\\Desktop\\BTP\\BTP codes\\Stories\\the_tiger_in_the_tunnel.txt\")\n",
    "file = open(r'C:\\Users\\Sourav\\BTP Code\\Harry_Potter\\v1hp', 'rb')\n",
    "document_list = pickle.load(file)\n",
    "file.close()\n",
    "clean_text=preprocess_data(document_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start,stop,step=2,12,1\n",
    "plot_graph(clean_text,start,stop,step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.929*\"harry\" + 0.168*\"said\" + 0.133*\"ron\" + 0.068*\"back\" + 0.061*\"dudley\" + 0.055*\"looked\" + 0.052*\"one\" + 0.048*\"could\" + 0.046*\"hermione\" + 0.044*\"hagrid\" + 0.043*\"quirrell\" + 0.043*\"uncle\" + 0.042*\"time\" + 0.040*\"like\" + 0.038*\"face\" + 0.038*\"vernon\" + 0.036*\"get\" + 0.035*\"know\" + 0.034*\"see\" + 0.033*\"though\" + 0.033*\"hand\" + 0.033*\"eye\" + 0.031*\"letter\" + 0.031*\"head\" + 0.031*\"looking\" + 0.029*\"around\" + 0.028*\"still\" + 0.027*\"look\" + 0.026*\"day\" + 0.026*\"dumbledore\" + 0.026*\"door\" + 0.025*\"think\" + 0.024*\"going\" + 0.024*\"potter\" + 0.024*\"thought\" + 0.024*\"aunt\" + 0.023*\"boy\" + 0.023*\"people\" + 0.023*\"next\" + 0.023*\"saw\" + 0.022*\"even\" + 0.022*\"something\" + 0.022*\"felt\" + 0.021*\"first\" + 0.021*\"go\" + 0.021*\"malfoy\" + 0.021*\"got\" + 0.021*\"room\" + 0.021*\"thing\" + 0.020*\"right\" + 0.020*\"mirror\" + 0.020*\"told\" + 0.019*\"turned\" + 0.019*\"professor\" + 0.019*\"petunia\" + 0.019*\"two\" + 0.019*\"never\" + 0.019*\"found\" + 0.019*\"dursleys\" + 0.019*\"snape\" + 0.018*\"anything\" + 0.018*\"come\" + 0.018*\"way\" + 0.018*\"trying\" + 0.018*\"quickly\" + 0.018*\"came\" + 0.017*\"asked\" + 0.017*\"mr\" + 0.017*\"knew\" + 0.017*\"took\" + 0.017*\"cloak\" + 0.017*\"yes\" + 0.017*\"would\" + 0.016*\"stared\" + 0.016*\"tried\" + 0.016*\"front\" + 0.015*\"behind\" + 0.015*\"good\" + 0.015*\"voice\" + 0.015*\"three\" + 0.015*\"floor\" + 0.015*\"gave\" + 0.014*\"heard\" + 0.014*\"went\" + 0.014*\"inside\" + 0.014*\"bed\" + 0.014*\"away\" + 0.014*\"troll\" + 0.014*\"started\" + 0.014*\"large\" + 0.014*\"mcgonagall\" + 0.014*\"arm\" + 0.014*\"suddenly\" + 0.013*\"cupboard\" + 0.013*\"wood\" + 0.013*\"quite\" + 0.013*\"filch\" + 0.013*\"made\" + 0.013*\"bit\" + 0.013*\"house\" + 0.013*\"later\" + 0.013*\"top\" + 0.013*\"air\" + 0.013*\"corridor\" + 0.013*\"much\" + 0.013*\"nose\" + 0.013*\"every\" + 0.013*\"left\" + 0.013*\"keep\" + 0.013*\"almost\" + 0.013*\"school\" + 0.013*\"gone\" + 0.013*\"last\" + 0.013*\"put\" + 0.012*\"wand\" + 0.012*\"take\" + 0.012*\"pulled\" + 0.012*\"foot\" + 0.012*\"walked\" + 0.012*\"pain\" + 0.012*\"little\" + 0.012*\"lot\" + 0.012*\"voldemort\" + 0.012*\"hall\" + 0.012*\"mind\" + 0.012*\"toward\" + 0.011*\"wizard\" + 0.011*\"hogwarts\" + 0.011*\"work\" + 0.011*\"neck\" + 0.011*\"kitchen\" + 0.011*\"neville\" + 0.011*\"new\" + 0.011*\"open\" + 0.011*\"another\" + 0.011*\"old\" + 0.011*\"feeling\" + 0.011*\"ball\" + 0.010*\"glass\" + 0.010*\"family\" + 0.010*\"hour\" + 0.010*\"life\" + 0.010*\"great\" + 0.010*\"saying\" + 0.010*\"dark\" + 0.010*\"end\" + 0.010*\"green\" + 0.010*\"seemed\" + 0.010*\"sure\" + 0.010*\"read\" + 0.010*\"half\" + 0.010*\"week\" + 0.010*\"enough\" + 0.010*\"thinking\" + 0.010*\"scar\" + 0.010*\"christmas\" + 0.010*\"note\" + 0.009*\"must\" + 0.009*\"lay\" + 0.009*\"slowly\" + 0.009*\"might\" + 0.009*\"caught\" + 0.009*\"table\" + 0.009*\"make\" + 0.009*\"tell\" + 0.009*\"want\" + 0.009*\"without\" + 0.009*\"far\" + 0.009*\"black\" + 0.009*\"famous\" + 0.009*\"oh\" + 0.009*\"sitting\" + 0.009*\"telling\" + 0.009*\"sweater\" + 0.009*\"cake\" + 0.009*\"yeh\" + 0.009*\"pier\" + 0.009*\"wanted\" + 0.009*\"ground\" + 0.009*\"else\" + 0.009*\"red\" + 0.009*\"owl\" + 0.009*\"morning\" + 0.008*\"ever\" + 0.008*\"reached\" + 0.008*\"given\" + 0.008*\"knee\" + 0.008*\"platform\" + 0.008*\"train\" + 0.008*\"passed\" + 0.008*\"straight\" + 0.008*\"turning\" + 0.008*\"whispered\" + 0.008*\"seamus\" + 0.008*\"knocked\" + 0.008*\"long\" + 0.008*\"cry\" + 0.008*\"holding\" + 0.008*\"stone\" + 0.008*\"managed\" + 0.008*\"held\" + 0.008*\"hair\" + 0.008*\"ear\" + 0.008*\"snake\" + 0.008*\"let\" + 0.008*\"tea\" + 0.008*\"furious\" + 0.008*\"name\" + 0.008*\"opened\" + 0.008*\"onto\" + 0.008*\"sat\" + 0.007*\"terrible\" + 0.007*\"wall\" + 0.007*\"lesson\" + 0.007*\"year\" + 0.007*\"seat\" + 0.007*\"stood\" + 0.007*\"find\" + 0.007*\"talking\" + 0.007*\"car\" + 0.007*\"night\" + 0.007*\"watched\" + 0.007*\"stop\" + 0.007*\"moment\" + 0.007*\"pocket\" + 0.007*\"often\" + 0.007*\"already\" + 0.007*\"set\" + 0.007*\"minute\" + 0.007*\"mouth\" + 0.007*\"book\" + 0.007*\"compartment\" + 0.007*\"realized\" + 0.007*\"speak\" + 0.007*\"catch\" + 0.007*\"gasped\" + 0.007*\"called\" + 0.007*\"shook\" + 0.007*\"yet\" + 0.007*\"picked\" + 0.007*\"threw\" + 0.007*\"say\" + 0.007*\"round\" + 0.007*\"happened\" + 0.007*\"believe\" + 0.006*\"fred\" + 0.006*\"curse\" + 0.006*\"someone\" + 0.006*\"angry\" + 0.006*\"brought\" + 0.006*\"rock\" + 0.006*\"yell\" + 0.006*\"dead\" + 0.006*\"yelled\" + 0.006*\"whole\" + 0.006*\"spent\" + 0.006*\"pasty\" + 0.006*\"window\" + 0.006*\"funny\" + 0.006*\"arrived\" + 0.006*\"hear\" + 0.006*\"package\" + 0.006*\"cold\" + 0.006*\"card\" + 0.006*\"broomstick\" + 0.006*\"heavy\" + 0.006*\"heart\" + 0.006*\"forward\" + 0.006*\"moved\" + 0.006*\"nearly\" + 0.006*\"use\" + 0.006*\"call\" + 0.006*\"leapt\" + 0.006*\"horror\" + 0.006*\"hard\" + 0.006*\"sight\" + 0.006*\"screamed\" + 0.006*\"fell\" + 0.006*\"leg\" + 0.006*\"ask\" + 0.006*\"swung\" + 0.006*\"watch\" + 0.006*\"stopped\" + 0.006*\"twin\" + 0.006*\"knocking\" + 0.006*\"word\" + 0.006*\"getting\" + 0.006*\"favorite\" + 0.006*\"library\" + 0.006*\"trunk\" + 0.006*\"liked\" + 0.006*\"seen\" + 0.006*\"fact\" + 0.006*\"thanks\" + 0.006*\"best\" + 0.006*\"invisibility\" + 0.005*\"except\" + 0.005*\"teeth\" + 0.005*\"staring\" + 0.005*\"high\" + 0.005*\"point\" + 0.005*\"parcel\" + 0.005*\"able\" + 0.005*\"nothing\" + 0.005*\"knowing\" + 0.005*\"small\" + 0.005*\"dear\" + 0.005*\"pig\" + 0.005*\"smile\" + 0.005*\"pair\" + 0.005*\"h\" + 0.005*\"pale\" + 0.005*\"envelope\" + 0.005*\"castle\" + 0.005*\"sleep\" + 0.005*\"chocolate\" + 0.005*\"idea\" + 0.005*\"really\" + 0.005*\"hundred\" + 0.005*\"kill\" + 0.005*\"sit\" + 0.005*\"ice\" + 0.005*\"stupid\" + 0.005*\"rolled\" + 0.005*\"exactly\" + 0.005*\"shoulder\" + 0.005*\"quidditch\" + 0.005*\"mail\" + 0.005*\"hidden\" + 0.005*\"surprise\" + 0.005*\"man\" + 0.005*\"grabbed\" + 0.005*\"mother\" + 0.005*\"along\" + 0.005*\"racing\" + 0.005*\"feel\" + 0.005*\"flat\" + 0.005*\"interested\" + 0.005*\"breakfast\" + 0.005*\"knobbly\" + 0.005*\"bacon\" + 0.005*\"white\" + 0.005*\"everyone\" + 0.005*\"stick\" + 0.005*\"fire\" + 0.005*\"either\" + 0.005*\"finally\" + 0.005*\"shriek\" + 0.005*\"listen\" + 0.005*\"tear\" + 0.005*\"spoke\" + 0.005*\"shiny\" + 0.005*\"four\" + 0.005*\"giant\" + 0.005*\"horrible\" + 0.005*\"past\" + 0.005*\"dinner\" + 0.005*\"true\" + 0.005*\"guard\" + 0.005*\"blanket\" + 0.005*\"club\" + 0.005*\"sound\" + 0.005*\"raised\" + 0.005*\"shut\" + 0.005*\"ran\" + 0.005*\"forgotten\" + 0.005*\"chess\" + 0.005*\"fight\" + 0.005*\"howling\" + 0.005*\"jumped\" + 0.005*\"stay\" + 0.005*\"bag\" + 0.005*\"clean\" + 0.005*\"move\" + 0.005*\"chaser\" + 0.005*\"muttered\" + 0.005*\"rounded\" + 0.005*\"added\" + 0.005*\"er\" + 0.005*\"empty\" + 0.005*\"hat\" + 0.005*\"class\" + 0.005*\"building\" + 0.005*\"golf\" + 0.005*\"subject\" + 0.005*\"redder\" + 0.005*\"amazement\" + 0.005*\"shouted\" + 0.004*\"changed\" + 0.004*\"thick\" + 0.004*\"noticed\" + 0.004*\"flitwick\" + 0.004*\"ter\" + 0.004*\"dragon\" + 0.004*\"worst\" + 0.004*\"panted\" + 0.004*\"better\" + 0.004*\"drive\" + 0.004*\"third\" + 0.004*\"taken\" + 0.004*\"zoo\" + 0.004*\"full\" + 0.004*\"birthday\" + 0.004*\"course\" + 0.004*\"gringotts\" + 0.004*\"stepped\" + 0.004*\"meeting\" + 0.004*\"case\" + 0.004*\"maybe\" + 0.004*\"well\" + 0.004*\"coming\" + 0.004*\"afternoon\" + 0.004*\"alone\" + 0.004*\"sent\" + 0.004*\"cup\" + 0.004*\"holiday\" + 0.004*\"figg\" + 0.004*\"chance\" + 0.004*\"haircut\" + 0.004*\"wearing\" + 0.004*\"standing\" + 0.004*\"laughing\" + 0.004*\"gang\" + 0.004*\"smiling\" + 0.004*\"huge\" + 0.004*\"frog\" + 0.004*\"side\" + 0.004*\"touch\" + 0.004*\"country\" + 0.004*\"sorry\" + 0.004*\"norbert\" + 0.004*\"purple\" + 0.004*\"brave\" + 0.004*\"done\" + 0.004*\"norris\" + 0.004*\"throwing\" + 0.004*\"bought\" + 0.004*\"low\" + 0.004*\"stand\" + 0.004*\"need\" + 0.004*\"making\" + 0.004*\"nice\" + 0.004*\"ripped\" + 0.004*\"girl\" + 0.004*\"broke\" + 0.004*\"disappeared\" + 0.004*\"brown\" + 0.004*\"turban\" + 0.004*\"wrong\" + 0.004*\"u\" + 0.004*\"instead\" + 0.004*\"aside\" + 0.004*\"lucky\" + 0.004*\"remember\" + 0.004*\"confused\" + 0.004*\"live\" + 0.004*\"handed\" + 0.004*\"stomach\" + 0.004*\"force\" + 0.004*\"paper\" + 0.004*\"friday\" + 0.004*\"notice\" + 0.004*\"anyone\" + 0.004*\"relieved\" + 0.004*\"help\" + 0.004*\"furiously\" + 0.004*\"second\" + 0.004*\"none\" + 0.004*\"strange\" + 0.004*\"surprised\" + 0.004*\"hedwig\" + 0.004*\"question\" + 0.004*\"perhaps\" + 0.004*\"taking\" + 0.004*\"pleased\" + 0.004*\"near\" + 0.004*\"passageway\" + 0.004*\"nasty\" + 0.004*\"luck\" + 0.004*\"yer\" + 0.004*\"crowd\" + 0.004*\"evening\" + 0.004*\"wondering\" + 0.004*\"stretched\" + 0.004*\"scribbled\" + 0.004*\"close\" + 0.004*\"bang\" + 0.004*\"television\" + 0.004*\"corner\" + 0.004*\"supposed\" + 0.004*\"light\" + 0.004*\"student\" + 0.004*\"mile\" + 0.004*\"throw\" + 0.004*\"pushing\" + 0.004*\"trust\" + 0.004*\"understand\" + 0.004*\"clock\" + 0.004*\"gallery\" + 0.004*\"blue\" + 0.004*\"robe\" + 0.004*\"power\" + 0.004*\"dangling\" + 0.004*\"across\" + 0.004*\"listening\" + 0.004*\"breaking\" + 0.004*\"punching\" + 0.004*\"lightning\" + 0.004*\"snout\" + 0.004*\"ordinary\" + 0.004*\"please\" + 0.004*\"blinding\" + 0.004*\"kept\" + 0.004*\"laughed\" + 0.004*\"egg\" + 0.004*\"hissed\" + 0.004*\"scared\" + 0.004*\"upstairs\" + 0.004*\"happy\" + 0.004*\"mistake\" + 0.004*\"written\" + 0.004*\"smell\" + 0.004*\"nostril\" + 0.004*\"lamp\" + 0.004*\"pushed\" + 0.003*\"stair\" + 0.003*\"dropped\" + 0.003*\"lost\" + 0.003*\"plate\" + 0.003*\"shout\" + 0.003*\"shrieked\" + 0.003*\"lunged\" + 0.003*\"landing\" + 0.003*\"agony\" + 0.003*\"coat\" + 0.003*\"forehead\" + 0.003*\"awake\" + 0.003*\"armor\" + 0.003*\"today\" + 0.003*\"start\" + 0.003*\"gryffindor\" + 0.003*\"nine\" + 0.003*\"climbed\" + 0.003*\"portrait\" + 0.003*\"known\" + 0.003*\"warmth\" + 0.003*\"mean\" + 0.003*\"several\" + 0.003*\"story\" + 0.003*\"flamel\" + 0.003*\"jump\" + 0.003*\"direction\" + 0.003*\"thin\" + 0.003*\"leaned\" + 0.003*\"leap\" + 0.003*\"argue\" + 0.003*\"tucked\" + 0.003*\"son\" + 0.003*\"seized\" + 0.003*\"lived\" + 0.003*\"nodded\" + 0.003*\"purpose\" + 0.003*\"headmistress\" + 0.003*\"wishing\" + 0.003*\"boa\" + 0.003*\"constrictor\" + 0.003*\"somehow\" + 0.003*\"reminded\" + 0.003*\"lock\" + 0.003*\"sounded\" + 0.003*\"alley\" + 0.003*\"diagon\" + 0.003*\"hated\" + 0.003*\"others\" + 0.003*\"quaffle\" + 0.003*\"cart\" + 0.003*\"rope\" + 0.003*\"council\" + 0.003*\"bank\" + 0.003*\"complain\" + 0.003*\"everything\" + 0.003*\"friend\" + 0.003*\"bigger\" + 0.003*\"privet\" + 0.003*\"calming\" + 0.003*\"least\" + 0.003*\"thousand\" + 0.003*\"sir\" + 0.003*\"began\" + 0.003*\"hit\" + 0.003*\"realizing\" + 0.003*\"always\" + 0.003*\"reach\" + 0.003*\"classroom\" + 0.003*\"ten\" + 0.003*\"dursley\" + 0.003*\"met\" + 0.003*\"madam\" + 0.003*\"c\" + 0.003*\"fluffy\" + 0.003*\"wait\" + 0.003*\"dad\" + 0.003*\"george\" + 0.003*\"loudly\" + 0.003*\"closed\" + 0.003*\"pointing\" + 0.003*\"mine\" + 0.003*\"five\" + 0.003*\"game\" + 0.003*\"bike\" + 0.003*\"crabbe\" + 0.003*\"goyle\" + 0.003*\"pile\" + 0.003*\"grow\" + 0.003*\"london\" + 0.003*\"prodded\" + 0.003*\"smaller\" + 0.003*\"halloween\" + 0.003*\"warning\" + 0.003*\"place\" + 0.003*\"entrance\" + 0.003*\"eating\" + 0.003*\"slept\" + 0.003*\"careful\" + 0.003*\"tired\" + 0.003*\"charm\" + 0.003*\"meet\" + 0.003*\"lying\" + 0.003*\"trophy\" + 0.003*\"sprinted\" + 0.003*\"give\" + 0.003*\"proper\" + 0.003*\"unless\" + 0.003*\"putting\" + 0.003*\"share\" + 0.003*\"stunned\" + 0.003*\"leaving\" + 0.003*\"whether\" + 0.003*\"possible\" + 0.003*\"rib\" + 0.003*\"run\" + 0.003*\"indeed\" + 0.003*\"answer\" + 0.003*\"tantrum\" + 0.003*\"difficult\" + 0.003*\"crate\" + 0.003*\"hardly\" + 0.003*\"month\" + 0.003*\"ink\" + 0.003*\"business\" + 0.003*\"pull\" + 0.003*\"fighting\" + 0.003*\"cream\" + 0.003*\"burst\" + 0.003*\"muggles\" + 0.003*\"beaming\" + 0.003*\"waved\" + 0.003*\"nerve\" + 0.003*\"hospital\" + 0.003*\"wing\" + 0.003*\"honestly\" + 0.003*\"grab\" + 0.003*\"sister\" + 0.003*\"slid\" + 0.003*\"common\" + 0.003*\"headed\" + 0.003*\"shot\" + 0.003*\"outside\" + 0.003*\"paused\" + 0.003*\"lap\" + 0.003*\"computer\" + 0.003*\"gotten\" + 0.003*\"nervous\" + 0.003*\"yellow\" + 0.003*\"snap\" + 0.003*\"wild\" + 0.003*\"baked\" + 0.003*\"stonewall\" + 0.003*\"shock\" + 0.003*\"world\" + 0.003*\"moaning\" + 0.003*\"lead\" + 0.003*\"following\" + 0.003*\"hurtled\" + 0.003*\"tapestry\" + 0.003*\"galloped\" + 0.003*\"doorpost\" + 0.003*\"bye\" + 0.003*\"doormat\" + 0.003*\"showed\" + 0.003*\"eleven\" + 0.003*\"followed\" + 0.003*\"breath\" + 0.003*\"gasping\" + 0.003*\"glanced\" + 0.003*\"uniform\" + 0.003*\"death\" + 0.003*\"hurry\" + 0.003*\"anxiously\" + 0.003*\"sheet\" + 0.003*\"rushed\" + 0.003*\"shop\" + 0.003*\"hung\" + 0.003*\"sandwich\" + 0.003*\"refuse\" + 0.002*\"grin\" + 0.002*\"weighed\" + 0.002*\"polite\" + 0.002*\"rule\" + 0.002*\"mixture\" + 0.002*\"filled\" + 0.002*\"lady\" + 0.002*\"visited\" + 0.002*\"fly\" + 0.002*\"winked\" + 0.002*\"hoping\" + 0.002*\"bathrobe\" + 0.002*\"cauldron\" + 0.002*\"desperate\" + 0.002*\"big\" + 0.002*\"fun\" + 0.002*\"real\" + 0.002*\"partner\" + 0.002*\"barrier\" + 0.002*\"hut\" + 0.002*\"piece\" + 0.002*\"running\" + 0.002*\"faced\" + 0.002*\"station\" + 0.002*\"hope\" + 0.002*\"losing\" + 0.002*\"motorcycle\" + 0.002*\"glad\" + 0.002*\"used\" + 0.002*\"turn\" + 0.002*\"smiled\" + 0.002*\"smelting\" + 0.002*\"sighed\" + 0.002*\"blood\" + 0.002*\"fat\" + 0.002*\"closer\" + 0.002*\"sharply\" + 0.002*\"feast\" + 0.002*\"short\" + 0.002*\"pointed\" + 0.002*\"shocked\" + 0.002*\"change\" + 0.002*\"twelve\" + 0.002*\"g\" + 0.002*\"f\" + 0.002*\"fast\" + 0.002*\"pumpkin\" + 0.002*\"nearer\" + 0.002*\"terror\" + 0.002*\"seven\" + 0.002*\"keeper\" + 0.002*\"waving\" + 0.002*\"raw\" + 0.002*\"pinning\" + 0.002*\"burned\" + 0.002*\"bewildered\" + 0.002*\"palm\" + 0.002*\"panic\" + 0.002*\"cheer\" + 0.002*\"blow\" + 0.002*\"wandering\" + 0.002*\"fudge\" + 0.002*\"crack\" + 0.002*\"promptly\" + 0.002*\"silent\" + 0.002*\"keyhole\" + 0.002*\"addressed\" + 0.002*\"return\" + 0.002*\"delayed\" + 0.002*\"may\" + 0.002*\"hiding\" + 0.002*\"tomorrow\" + 0.002*\"woke\" + 0.002*\"curiously\" + 0.002*\"reflection\" + 0.002*\"squashy\" + 0.002*\"pressed\" + 0.002*\"wrist\" + 0.002*\"hanging\" + 0.002*\"vanished\" + 0.002*\"exploded\" + 0.002*\"living\" + 0.002*\"delighted\" + 0.002*\"worry\" + 0.002*\"kind\" + 0.002*\"key\" + 0.002*\"pack\" + 0.002*\"clearly\" + 0.002*\"wonderful\" + 0.002*\"emerald\" + 0.002*\"bedroom\" + 0.002*\"needed\" + 0.002*\"remembering\" + 0.002*\"appearance\" + 0.002*\"bat\" + 0.002*\"seems\" + 0.002*\"creep\" + 0.002*\"roll\" + 0.002*\"stuck\" + 0.002*\"friendly\" + 0.002*\"meant\" + 0.002*\"entered\" + 0.002*\"marble\" + 0.002*\"excited\" + 0.002*\"terrified\" + 0.002*\"cracker\" + 0.002*\"received\" + 0.002*\"warned\" + 0.002*\"rather\" + 0.002*\"mouse\" + 0.002*\"tasted\" + 0.002*\"hole\" + 0.002*\"talked\" + 0.002*\"loaded\" + 0.002*\"tore\" + 0.002*\"windowsill\" + 0.002*\"six\" + 0.002*\"fang\" + 0.002*\"underneath\" + 0.002*\"toilet\" + 0.002*\"safely\" + 0.002*\"waited\" + 0.002*\"answered\" + 0.002*\"knowin\" + 0.002*\"torn\" + 0.002*\"suit\" + 0.002*\"respect\" + 0.002*\"blazing\" + 0.002*\"proud\" + 0.002*\"kicked\" + 0.002*\"jerked\" + 0.002*\"single\" + 0.002*\"working\" + 0.002*\"quarter\" + 0.002*\"laid\" + 0.002*\"gently\" + 0.002*\"doorstep\" + 0.002*\"eyebrow\" + 0.002*\"bright\" + 0.002*\"sudden\" + 0.002*\"trembling\" + 0.002*\"sugar\" + 0.002*\"rest\" + 0.002*\"damp\" + 0.002*\"frying\" + 0.002*\"dreamed\" + 0.002*\"sock\" + 0.002*\"backward\" + 0.002*\"cheek\" + 0.002*\"giggled\" + 0.002*\"lopsided\" + 0.002*\"kissing\" + 0.002*\"blushed\" + 0.002*\"wine\" + 0.002*\"trip\" + 0.002*\"part\" + 0.002*\"annoyed\" + 0.002*\"public\" + 0.002*\"murmured\" + 0.002*\"tongue\" + 0.002*\"upside\" + 0.002*\"suggested\" + 0.002*\"escape\" + 0.002*\"shivered\" + 0.002*\"jot\" + 0.002*\"dry\" + 0.002*\"relief\" + 0.002*\"team\" + 0.002*\"wash\" + 0.002*\"chair\" + 0.002*\"winning\" + 0.002*\"hate\" + 0.002*\"tail\" + 0.002*\"box\" + 0.002*\"legend\" + 0.002*\"future\" + 0.002*\"child\" + 0.002*\"bolt\" + 0.002*\"tank\" + 0.002*\"spot\" + 0.002*\"wrapped\" + 0.002*\"sprang\" + 0.002*\"finnigan\" + 0.002*\"locked\" + 0.002*\"bowl\" + 0.002*\"score\" + 0.002*\"step\" + 0.002*\"faded\" + 0.002*\"climbing\" + 0.002*\"baby\" + 0.002*\"ignored\" + 0.002*\"shake\" + 0.002*\"binding\" + 0.002*\"woken\" + 0.002*\"quietly\" + 0.002*\"peered\" + 0.002*\"show\" + 0.002*\"granger\" + 0.002*\"odd\" + 0.002*\"miss\" + 0.002*\"tree\" + 0.002*\"scream\" + 0.002*\"grinding\" + 0.002*\"clothes\" + 0.002*\"leave\" + 0.002*\"special\" + 0.002*\"wig\" + 0.002*\"angel\" + 0.002*\"lumpy\" + 0.002*\"suppose\" + 0.002*\"shaking\" + 0.002*\"allowed\" + 0.002*\"pink\" + 0.002*\"ridiculous\" + 0.002*\"blistering\" + 0.002*\"pinched\" + 0.002*\"secret\" + 0.002*\"spend\" + 0.002*\"cousin\" + 0.002*\"hushed\" + 0.002*\"beside\" + 0.002*\"milk\" + 0.002*\"bottle\" + 0.002*\"quill\" + 0.002*\"address\" + 0.002*\"fall\" + 0.002*\"chased\" + 0.002*\"upset\" + 0.002*\"revenge\" + 0.002*\"dreading\" + 0.002*\"sort\" + 0.002*\"mountain\" + 0.002*\"worried\" + 0.002*\"laugh\" + 0.002*\"walk\" + 0.002*\"dying\" + 0.002*\"since\" + 0.002*\"hitting\" + 0.002*\"eagerly\" + 0.002*\"asking\" + 0.002*\"apart\" + 0.002*\"video\" + 0.002*\"remote\" + 0.002*\"control\" + 0.002*\"airplane\" + 0.002*\"camera\" + 0.002*\"reminds\" + 0.002*\"witch\" + 0.002*\"skin\" + 0.002*\"possibly\" + 0.002*\"pulling\" + 0.002*\"charlie\" + 0.002*\"slipped\" + 0.002*\"coolly\" + 0.002*\"giving\" + 0.002*\"asleep\" + 0.002*\"owlery\" + 0.002*\"calm\" + 0.002*\"james\" + 0.002*\"lily\" + 0.002*\"fungi\" + 0.002*\"herb\" + 0.002*\"magical\" + 0.002*\"delight\" + 0.002*\"many\" + 0.002*\"garden\" + 0.002*\"seem\" + 0.002*\"decided\" + 0.002*\"present\" + 0.002*\"contact\" + 0.002*\"practice\" + 0.002*\"concentrating\" + 0.002*\"scabbers\" + 0.002*\"whisker\" + 0.002*\"lighter\" + 0.002*\"toast\" + 0.002*\"finger\" + 0.002*\"telephone\" + 0.002*\"nimbus\" + 0.002*\"luminous\" + 0.002*\"together\" + 0.002*\"heel\" + 0.002*\"mark\" + 0.002*\"broken\" + 0.001*\"hurried\" + 0.001*\"dashing\" + 0.001*\"tipped\" + 0.001*\"desperately\" + 0.001*\"wished\" + 0.001*\"hotel\" + 0.001*\"bill\" + 0.001*\"postcard\" + 0.001*\"cat\" + 0.001*\"gray\" + 0.001*\"anyway\"')]\n"
     ]
    }
   ],
   "source": [
    "number_of_topics=1\n",
    "words=1000\n",
    "model=create_gensim_lsa_model(clean_text,number_of_topics,words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(r'C:\\Users\\Sourav\\BTP Code\\Harry_Potter\\lsaHP1_1000', 'wb')\n",
    "pickle.dump(model, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.042*\"harri\" + 0.017*\"ron\" + 0.013*\"said\" + 0.008*\"look\" + 0.005*\"turn\" + 0.005*\"back\" + 0.005*\"face\" + 0.005*\"could\" + 0.005*\"ask\" + 0.005*\"one\"'), (1, '0.087*\"harri\" + 0.017*\"said\" + 0.012*\"look\" + 0.012*\"ron\" + 0.008*\"back\" + 0.006*\"go\" + 0.006*\"dudley\" + 0.005*\"get\" + 0.005*\"could\" + 0.005*\"hermion\"')]\n"
     ]
    }
   ],
   "source": [
    "number_of_topics=2\n",
    "words=10\n",
    "ldamodel=create_gensim_lda_model(clean_text,number_of_topics,words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sourav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    nominator = np.dot(a, b)\n",
    "    \n",
    "    a_norm = np.sqrt(np.sum(a**2))\n",
    "    b_norm = np.sqrt(np.sum(b**2))\n",
    "    \n",
    "    denominator = a_norm * b_norm\n",
    "    \n",
    "    cosine_similarity = nominator / denominator\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
