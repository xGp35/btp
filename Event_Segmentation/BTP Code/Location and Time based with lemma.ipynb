{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from empath import Empath\n",
    "import requests\n",
    "import json\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_category(self,name,seeds,model=\"fiction\",size=100,write=True):\n",
    "    resp = requests.post(self.backend_url + \"/create_category\", json={\"terms\":seeds,\"size\":size,\"model\":model})\n",
    "    results = json.loads(resp.text)\n",
    "    lemma_words = list()\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    for word in results:\n",
    "        lemma_words.append(lemmatizer.lemmatize(word))\n",
    "    self.cats[name] = list(set(lemma_words))\n",
    "    if write:\n",
    "        with open(self.base_dir+\"/data/user/\"+name+\".empath\",\"w\") as f:\n",
    "            f.write(\"\\t\".join([name]+results))\n",
    "\n",
    "Empath.create_lemma_category = create_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "lexicon = Empath()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from stanza.server import CoreNLPClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lexicons(rb,lv,fp,ct):\n",
    "    lexicon.create_lemma_category(\"religious_buildings\", [\"church\",\"mosque\", \"temple\"], model=\"fiction\", size = rb)\n",
    "    lexicon.create_lemma_category(\"loc_verbs\", [\"arrive\", \"visit\", \"travel\", \"return\"], model = \"fiction\", size= lv)\n",
    "    lexicon.create_lemma_category(\"fictional_places\", [\"place\",\"buildings\"], model =\"fiction\", size =fp)\n",
    "    lexicon.create_lemma_category(\"custom_times\", [\"once_upon_a_time\", \"next_day\",\"that_evening\"], size = ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_lexicons(30,14,300,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the_story_of_the_merchant_son', 'the_aged_mother', 'the_thief_and_the_brahmins', 'the_monkey_and_the_crocodile', 'the_monkey_the_wedge']\n"
     ]
    }
   ],
   "source": [
    "story_names = []\n",
    "file = open(\"D:\\Jupyter\\BTP\\Panchatantra\\Storynames.txt\")\n",
    "file_story_names = file.readlines()\n",
    "for name in file_story_names:\n",
    "    story_names.append(name.strip('\\n'))\n",
    "file.close()\n",
    "print(story_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_story(Storyname):\n",
    "    file = open(\"D:\\Jupyter\\BTP\\Panchatantra\\\\\"+Storyname+'.txt')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "def annotate_story(text):\n",
    "    with CoreNLPClient(annotators = ['tokenize','ssplit'],\n",
    "        memory='5G', be_quiet=True, outputFormat = 'json', max_char_length=500000, timeout=36000000) as client:\n",
    "        annotated_story = client.annotate(text)\n",
    "    return annotated_story\n",
    "def open_and_annotate(Storyname):\n",
    "    text = open_story(Storyname)\n",
    "    annotated_story = annotate_story(text)\n",
    "    return text, annotated_story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def events_by_location_and_time(text,ann):\n",
    "    \"\"\"\n",
    "    Non-hierarchy model\n",
    "    \"\"\"\n",
    "    #This function finds sum of dictionary returned by lexicon.analyze i.e., it finds the presence of location words.\n",
    "    def sum_of_locs_dict(dictionary):\n",
    "        sum_ = 0\n",
    "        for key in dictionary.keys():\n",
    "            sum_ = sum_ + dictionary[key]\n",
    "        return sum_\n",
    "       \n",
    "    lexicon = Empath()   #Part of code used to bring Empath in\n",
    "    locations_dict = dict()     #Dictionary that holds\n",
    "    location = \"Unknown\"    #The variable place will hold latest location word.\n",
    "                            #It is initilized to \"unknown\" beacuse till now we haven't encountered any location word.\n",
    "    loaction_by_sentence = []\n",
    "    location_to_number = dict() # Convert location words to numbers for better representation\n",
    "    loc_num = 0 # Will be used to put location words as numbers in the location_to_number dict\n",
    "    total_sentences = 0\n",
    "    \n",
    "    #Take each sentence of the story one by one (ann.sentence returns individual sentences of the story as objects)\n",
    "    for i, sentence in enumerate(ann.sentence):\n",
    "        # Remove comma and fullstop beacuse lexicon.analyze cannot identify words if they are followd by a fullstop or comma.\n",
    "        # text[characterOffsetBegin:characterOffserEnd] is the actual sentence (as a string) of the sentence object returned\n",
    "        sentence_for_empath = text[sentence.characterOffsetBegin:sentence.characterOffsetEnd].replace(\", \",\" \").replace(\".\",\"\").replace(\"-\",\" \").replace(\"?\",\"\").replace(\"!\",\"\").replace(\":\",\" \")\n",
    "        #Lemmatize the words you encounter for better identification when being analysed by lexicon.analyze\n",
    "        #May be commented out because lexicon.create_category does not give good words when singular words are used\n",
    "        sentence_for_empath = lemmatize_sentence(sentence_for_empath) # Sentences are all lemmatized now\n",
    "        # Analyze the things\n",
    "        lexicon_locations_dict = lexicon.analyze(sentence_for_empath,\n",
    "                                                categories=[\"religious_buildings\", \"loc_verbs\", \"fictional_places\", \"custom_times\"])\n",
    "        \n",
    "        s = sum_of_locs_dict(lexicon_locations_dict)\n",
    "        if s>0:\n",
    "            words = sentence_for_empath.split(\" \")\n",
    "            # Find if place is same as previous\n",
    "            for word in words:\n",
    "                # If the word is a location word\n",
    "                if sum_of_locs_dict(lexicon.analyze(word,\n",
    "                                                   categories=[\"religious_buildings\", \"loc_verbs\", \"fictional_places\", \"custom_times\"]))>0:\n",
    "                    #if new location word encountered is the same as the last location word encountered\n",
    "                    if word == location:\n",
    "                        break\n",
    "                    else:\n",
    "                        location = word\n",
    "                        if location in locations_dict:\n",
    "                            location+=\"1\"\n",
    "                        locations_dict[location]=[i]\n",
    "                        location_to_number[location] = loc_num\n",
    "                        loc_num += 1\n",
    "        else: \n",
    "            if location not in locations_dict:\n",
    "                locations_dict[location] = list()\n",
    "            locations_dict[location].append(i)\n",
    "        total_sentences = i\n",
    "    return locations_dict, location_to_number, total_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_event_list(locations_dict):\n",
    "    events= []\n",
    "    for location in locations_dict:\n",
    "        events.append(locations_dict[location][0])\n",
    "    events.sort()\n",
    "    events.append(total_sentences)\n",
    "    while events[0] == 0:\n",
    "        del events[0]\n",
    "    return set(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-16 01:38:34 INFO: Writing properties to tmp file: corenlp_server-8416e896f90440c7.props\n",
      "2021-03-16 01:38:35 INFO: Starting server with command: java -Xmx5G -cp C:\\Users\\Giri\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000000 -threads 5 -maxCharLength 500000 -quiet True -serverProperties corenlp_server-8416e896f90440c7.props -annotators tokenize,ssplit -preload -outputFormat serialized\n",
      "2021-03-16 01:38:36 INFO: Writing properties to tmp file: corenlp_server-2b0f74dd77aa4ae5.props\n",
      "2021-03-16 01:38:36 INFO: Starting server with command: java -Xmx5G -cp C:\\Users\\Giri\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000000 -threads 5 -maxCharLength 500000 -quiet True -serverProperties corenlp_server-2b0f74dd77aa4ae5.props -annotators tokenize,ssplit -preload -outputFormat serialized\n",
      "2021-03-16 01:38:38 INFO: Writing properties to tmp file: corenlp_server-2dfa9d027d3b4e23.props\n",
      "2021-03-16 01:38:38 INFO: Starting server with command: java -Xmx5G -cp C:\\Users\\Giri\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000000 -threads 5 -maxCharLength 500000 -quiet True -serverProperties corenlp_server-2dfa9d027d3b4e23.props -annotators tokenize,ssplit -preload -outputFormat serialized\n",
      "2021-03-16 01:38:40 INFO: Writing properties to tmp file: corenlp_server-72e40f856f1f43d1.props\n",
      "2021-03-16 01:38:40 INFO: Starting server with command: java -Xmx5G -cp C:\\Users\\Giri\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000000 -threads 5 -maxCharLength 500000 -quiet True -serverProperties corenlp_server-72e40f856f1f43d1.props -annotators tokenize,ssplit -preload -outputFormat serialized\n",
      "2021-03-16 01:38:41 INFO: Writing properties to tmp file: corenlp_server-4427f14777414e70.props\n",
      "2021-03-16 01:38:41 INFO: Starting server with command: java -Xmx5G -cp C:\\Users\\Giri\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000000 -threads 5 -maxCharLength 500000 -quiet True -serverProperties corenlp_server-4427f14777414e70.props -annotators tokenize,ssplit -preload -outputFormat serialized\n"
     ]
    }
   ],
   "source": [
    "my_list_of_events = []\n",
    "for i,name in enumerate(story_names):\n",
    "    text , annotated_story = open_and_annotate(name)\n",
    "    locations_dict, location_number_map, total_sentences = events_by_location_and_time(text, annotated_story)\n",
    "    my_list_of_events.append(construct_event_list(locations_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{6, 7, 8, 10, 15, 27, 30, 33, 46, 51, 62, 65},\n",
       " {1, 10, 11, 15, 17, 23, 28, 30, 54},\n",
       " {2, 8, 16, 18, 23, 25, 48},\n",
       " {9, 24, 47, 56, 60},\n",
       " {1, 2, 4, 11}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list_of_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
